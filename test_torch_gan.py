#### BELOW ENTIRELY GENERATED BY CHATGPT!

import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np
from torch.utils.data import DataLoader, TensorDataset, random_split

# Assuming Raman spectra are stored in numpy arrays
raman_spectra = np.load('raman_spectra.npy')

# Convert numpy arrays to PyTorch tensors
real_spectra = torch.tensor(raman_spectra, dtype=torch.float32)

# Create a PyTorch dataset
dataset = TensorDataset(real_spectra)

# Create a data loader
batch_size = 32
data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)

# Define the Generator network
class Generator(nn.Module):
    def __init__(self, noise_dim, output_size):
        super(Generator, self).__init__()
        self.fc1 = nn.Linear(noise_dim, 128)
        self.fc2 = nn.Linear(128, 256)
        self.fc3 = nn.Linear(256, 512)
        self.fc4 = nn.Linear(512, output_size)
        self.relu = nn.ReLU()
        self.tanh = nn.Tanh()  # Use Tanh to get outputs in the range [-1, 1]

    def forward(self, x):
        x = self.relu(self.fc1(x))
        x = self.relu(self.fc2(x))
        x = self.relu(self.fc3(x))
        x = self.tanh(self.fc4(x))  # Output scaled to [-1, 1]
        return x

# Define the Discriminator network
class Discriminator(nn.Module):
    def __init__(self, input_size):
        super(Discriminator, self).__init__()
        self.fc1 = nn.Linear(input_size, 512)
        self.fc2 = nn.Linear(512, 256)
        self.fc3 = nn.Linear(256, 128)
        self.fc4 = nn.Linear(128, 1)
        self.relu = nn.ReLU()
        self.sigmoid = nn.Sigmoid()  # Output probability in the range [0, 1]

    def forward(self, x):
        x = self.relu(self.fc1(x))
        x = self.relu(self.fc2(x))
        x = self.relu(self.fc3(x))
        x = self.sigmoid(self.fc4(x))
        return x

# Initialize the models
noise_dim = 100  # Dimension of the noise vector
output_size = raman_spectra.shape[1]  # Length of the Raman spectrum
generator = Generator(noise_dim, output_size)
discriminator = Discriminator(output_size)

# Loss function and optimizers
criterion = nn.BCELoss()
optimizer_g = optim.Adam(generator.parameters(), lr=0.0002)
optimizer_d = optim.Adam(discriminator.parameters(), lr=0.0002)

# Training loop
num_epochs = 100
for epoch in range(num_epochs):
    for real_spectra in data_loader:
        real_spectra = real_spectra[0]
        batch_size = real_spectra.size(0)

        # Train Discriminator
        optimizer_d.zero_grad()

        # Real spectra
        labels_real = torch.ones(batch_size, 1)
        outputs_real = discriminator(real_spectra)
        loss_real = criterion(outputs_real, labels_real)

        # Fake spectra
        noise = torch.randn(batch_size, noise_dim)
        fake_spectra = generator(noise)
        labels_fake = torch.zeros(batch_size, 1)
        outputs_fake = discriminator(fake_spectra.detach())
        loss_fake = criterion(outputs_fake, labels_fake)

        # Backpropagation and optimization
        loss_d = loss_real + loss_fake
        loss_d.backward()
        optimizer_d.step()

        # Train Generator
        optimizer_g.zero_grad()

        noise = torch.randn(batch_size, noise_dim)
        fake_spectra = generator(noise)
        labels_real = torch.ones(batch_size, 1)  # We want the generator to fool the discriminator
        outputs_fake = discriminator(fake_spectra)
        loss_g = criterion(outputs_fake, labels_real)

        loss_g.backward()
        optimizer_g.step()

    print(f'Epoch [{epoch+1}/{num_epochs}], Loss D: {loss_d.item():.4f}, Loss G: {loss_g.item():.4f}')

# Save the trained models
torch.save(generator.state_dict(), 'generator.pth')
torch.save(discriminator.state_dict(), 'discriminator.pth')
